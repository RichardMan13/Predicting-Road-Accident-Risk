{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a19ae1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Welcome to the 2025 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month.\n",
    "\n",
    "Your Goal: Predict the likelihood of accidents on different types of roads.\n",
    "\n",
    "For this Playground Series challenge, we have teamed up with Stack Overflow to give you a two-part challenge. The Stack Overflow Challenge is the second part and builds upon this one by having participants develop a web application. We encourage you to check out the Stack Overflow Challenge!\n",
    "\n",
    "If you complete both challenges, we’ll recognize your breadth of skills with a special “Code Scientist” badge which will appear on both Kaggle and Stack Overflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573373bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508f6ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Simulated Roads Accident dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af633d0",
   "metadata": {},
   "source": [
    "# Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24fd5231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (517754, 12)\n",
      "Target shape: (517754,)\n",
      "Test data shape: (172585, 12)\n",
      "\n",
      "Applying label encoding...\n",
      "Encoded training data shape: (517754, 12)\n",
      "Encoded test data shape: (172585, 12)\n",
      "\n",
      "Final training set shape: (414203, 12)\n",
      "Final validation set shape: (103551, 12)\n",
      "\n",
      "1. Training XGBoost with Best Parameters\n",
      "----------------------------------------\n",
      "XGBoost parameters: {'device': 'cuda', 'random_state': 42, 'enable_categorical': True, 'max_depth': 7, 'learning_rate': 0.01, 'n_estimators': 1000}\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repositorios_pessoais\\Predicting-Road-Accident-Risk\\.venv\\Lib\\site-packages\\xgboost\\core.py:729: UserWarning: [18:21:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Final Performance:\n",
      "MSE: 0.003156\n",
      "RMSE: 0.056178\n",
      "MAE: 0.043590\n",
      "R²: 0.885703\n",
      "\n",
      "2. Training KNN with Best Parameters\n",
      "----------------------------------------\n",
      "KNN parameters: {'n_neighbors': 20}\n",
      "Training KNN...\n",
      "\n",
      "KNN Final Performance:\n",
      "MSE: 0.004129\n",
      "RMSE: 0.064256\n",
      "MAE: 0.050093\n",
      "R²: 0.850473\n",
      "\n",
      "Final Model Comparison:\n",
      "==============================\n",
      "Model      RMSE         R²          \n",
      "-----------------------------------\n",
      "XGBoost    0.056178     0.885703    \n",
      "KNN        0.064256     0.850473    \n",
      "\n",
      "Best model: XGBoost\n",
      "Best RMSE: 0.056178\n",
      "\n",
      "Models trained successfully with best parameters!\n"
     ]
    }
   ],
   "source": [
    "# Retrain XGBoost and KNN with Best Parameters Found\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Prepare features and target using processed data\n",
    "X = train.drop(['id', 'accident_risk'], axis=1)\n",
    "y = train['accident_risk']\n",
    "X_test_final = test.drop(['id'], axis=1)\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Test data shape: {X_test_final.shape}\")\n",
    "\n",
    "# Apply encoding to all data\n",
    "print(\"\\nApplying label encoding...\")\n",
    "X_encoded = X.copy()\n",
    "X_test_encoded = X_test_final.copy()\n",
    "\n",
    "# Use the same label encoders from previous training\n",
    "# cols = ['num_lanes', 'num_reported_accidents']\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('onehot', OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), cols)\n",
    "#     ],\n",
    "#     remainder='passthrough'\n",
    "# )\n",
    "# # Fit e transform\n",
    "# X_encoded = preprocessor.fit_transform(X_encoded)\n",
    "# X_test_encoded = preprocessor.transform(X_test_encoded)\n",
    "\n",
    "# feature_names = preprocessor.get_feature_names_out()\n",
    "# feature_names = [x.split(\"__\")[-1] for x in feature_names]\n",
    "# X_encoded = pd.DataFrame(X_encoded, columns=feature_names)\n",
    "# X_test_encoded = pd.DataFrame(X_test_encoded, columns=feature_names)\n",
    "\n",
    "categorical_cols = ['road_type', 'lighting', 'weather', 'time_of_day']\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "    X_test_encoded[col] = le.transform(X_test_encoded[col].astype(str))\n",
    "\n",
    "# X_encoded['speed_limit'] = X_encoded['speed_limit'].astype(int)\n",
    "# X_encoded['curvature'] = X_encoded['curvature'].astype(float)\n",
    "\n",
    "# X_test_encoded['speed_limit'] = X_test_encoded['speed_limit'].astype(int)\n",
    "# X_test_encoded['curvature'] = X_test_encoded['curvature'].astype(float)\n",
    "\n",
    "print(f\"Encoded training data shape: {X_encoded.shape}\")\n",
    "print(f\"Encoded test data shape: {X_test_encoded.shape}\")\n",
    "\n",
    "# Split data for validation\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal training set shape: {X_train_final.shape}\")\n",
    "print(f\"Final validation set shape: {X_val_final.shape}\")\n",
    "\n",
    "# Train XGBoost with best parameters\n",
    "print(\"\\n1. Training XGBoost with Best Parameters\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Best XGBoost parameters from previous training\n",
    "best_xgb_params = {\n",
    "    'device': 'cuda',\n",
    "    'random_state': 42,\n",
    "    'enable_categorical': True,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 1000\n",
    "}\n",
    "\n",
    "print(f\"XGBoost parameters: {best_xgb_params}\")\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_final = XGBRegressor(**best_xgb_params)\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_final.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Make predictions on validation set\n",
    "xgb_val_pred = xgb_final.predict(X_val_final)\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_final_mse = mean_squared_error(y_val_final, xgb_val_pred)\n",
    "xgb_final_mae = mean_absolute_error(y_val_final, xgb_val_pred)\n",
    "xgb_final_r2 = r2_score(y_val_final, xgb_val_pred)\n",
    "xgb_final_rmse = np.sqrt(xgb_final_mse)\n",
    "\n",
    "print(f\"\\nXGBoost Final Performance:\")\n",
    "print(f\"MSE: {xgb_final_mse:.6f}\")\n",
    "print(f\"RMSE: {xgb_final_rmse:.6f}\")\n",
    "print(f\"MAE: {xgb_final_mae:.6f}\")\n",
    "print(f\"R²: {xgb_final_r2:.6f}\")\n",
    "\n",
    "# Train KNN with best parameters\n",
    "print(\"\\n2. Training KNN with Best Parameters\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Best KNN parameters from previous training\n",
    "best_knn_params = {'n_neighbors': 20}\n",
    "\n",
    "print(f\"KNN parameters: {best_knn_params}\")\n",
    "\n",
    "# Create KNN pipeline with StandardScaler\n",
    "knn_pipeline_final = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor(**best_knn_params))\n",
    "])\n",
    "\n",
    "print(\"Training KNN...\")\n",
    "knn_pipeline_final.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Make predictions on validation set\n",
    "knn_val_pred = knn_pipeline_final.predict(X_val_final)\n",
    "\n",
    "# Calculate metrics\n",
    "knn_final_mse = mean_squared_error(y_val_final, knn_val_pred)\n",
    "knn_final_mae = mean_absolute_error(y_val_final, knn_val_pred)\n",
    "knn_final_r2 = r2_score(y_val_final, knn_val_pred)\n",
    "knn_final_rmse = np.sqrt(knn_final_mse)\n",
    "\n",
    "print(f\"\\nKNN Final Performance:\")\n",
    "print(f\"MSE: {knn_final_mse:.6f}\")\n",
    "print(f\"RMSE: {knn_final_rmse:.6f}\")\n",
    "print(f\"MAE: {knn_final_mae:.6f}\")\n",
    "print(f\"R²: {knn_final_r2:.6f}\")\n",
    "\n",
    "# xgb_final = xgb_final.fit(X_encoded, y)\n",
    "# knn_pipeline_final = knn_pipeline_final.fit(X_encoded, y)\n",
    "\n",
    "# xgb_val_pred = xgb_final.predict(X_test_encoded)\n",
    "# knn_val_pred = knn_pipeline_final.predict(X_test_encoded)\n",
    "\n",
    "# Store final models and results\n",
    "final_models = {\n",
    "    'XGBoost': {\n",
    "        'model': xgb_final,\n",
    "        'mse': xgb_final_mse,\n",
    "        'rmse': xgb_final_rmse,\n",
    "        'mae': xgb_final_mae,\n",
    "        'r2': xgb_final_r2,\n",
    "        'predictions': xgb_val_pred\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': knn_pipeline_final,\n",
    "        'mse': knn_final_mse,\n",
    "        'rmse': knn_final_rmse,\n",
    "        'mae': knn_final_mae,\n",
    "        'r2': knn_final_r2,\n",
    "        'predictions': knn_val_pred\n",
    "    }\n",
    "}\n",
    "\n",
    "# Compare final models\n",
    "print(f\"\\nFinal Model Comparison:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"{'Model':<10} {'RMSE':<12} {'R²':<12}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'XGBoost':<10} {xgb_final_rmse:<12.6f} {xgb_final_r2:<12.6f}\")\n",
    "print(f\"{'KNN':<10} {knn_final_rmse:<12.6f} {knn_final_r2:<12.6f}\")\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = 'XGBoost' if xgb_final_rmse < knn_final_rmse else 'KNN'\n",
    "best_model_rmse = min(xgb_final_rmse, knn_final_rmse)\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Best RMSE: {best_model_rmse:.6f}\")\n",
    "\n",
    "print(f\"\\nModels trained successfully with best parameters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5311af38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing Ensemble Methods for KNN + XGBoost...\n",
      "============================================================\n",
      "\n",
      "1. Weighted Average Ensemble\n",
      "------------------------------\n",
      "XGBoost RMSE: 0.0562 - Weight: 0.5335\n",
      "KNN RMSE: 0.0643 - Weight: 0.4665\n",
      "Total weight: 1.0000\n",
      "\n",
      "Weighted Ensemble Validation Performance:\n",
      "MSE: 0.0034\n",
      "MAE: 0.0452\n",
      "R²: 0.8780\n",
      "\n",
      "2. Stacking Regressor Ensemble\n",
      "------------------------------\n",
      "Training Stacking Regressor...\n",
      "\n",
      "Stacking Regressor Validation Performance:\n",
      "MSE: 0.0032\n",
      "MAE: 0.0436\n",
      "R²: 0.8857\n",
      "\n",
      "Ensemble implementation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Ensemble Methods: Combining KNN and XGBoost\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Implementing Ensemble Methods for KNN + XGBoost...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Method 1: Weighted Average Ensemble\n",
    "print(\"\\n1. Weighted Average Ensemble\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Calculate weights based on RMSE (inverse relationship)\n",
    "xgb_rmse = final_models['XGBoost']['rmse']\n",
    "knn_rmse = final_models['KNN']['rmse']\n",
    "\n",
    "# Calculate weights based on RMSE (inverse relationship)\n",
    "# Lower RMSE should get higher weight\n",
    "xgb_weight = 1 / xgb_rmse\n",
    "knn_weight = 1 / knn_rmse\n",
    "\n",
    "# Normalize weights\n",
    "total_weight = xgb_weight + knn_weight\n",
    "xgb_weight = xgb_weight / total_weight\n",
    "knn_weight = knn_weight / total_weight\n",
    "\n",
    "print(f\"XGBoost RMSE: {xgb_rmse:.4f} - Weight: {xgb_weight:.4f}\")\n",
    "print(f\"KNN RMSE: {knn_rmse:.4f} - Weight: {knn_weight:.4f}\")\n",
    "print(f\"Total weight: {xgb_weight + knn_weight:.4f}\")\n",
    "\n",
    "# Create weighted ensemble predictions on validation set\n",
    "weighted_val_predictions = (xgb_weight * final_models['XGBoost']['predictions'] + \n",
    "                           knn_weight * final_models['KNN']['predictions'])\n",
    "\n",
    "# Calculate ensemble metrics\n",
    "ensemble_mse = mean_squared_error(y_val_final, weighted_val_predictions)\n",
    "ensemble_mae = mean_absolute_error(y_val_final, weighted_val_predictions)\n",
    "ensemble_r2 = r2_score(y_val_final, weighted_val_predictions)\n",
    "\n",
    "print(f\"\\nWeighted Ensemble Validation Performance:\")\n",
    "print(f\"MSE: {ensemble_mse:.4f}\")\n",
    "print(f\"MAE: {ensemble_mae:.4f}\")\n",
    "print(f\"R²: {ensemble_r2:.4f}\")\n",
    "\n",
    "# Method 2: Stacking Regressor\n",
    "print(\"\\n2. Stacking Regressor Ensemble\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create base models\n",
    "base_models = [\n",
    "    ('xgb', final_models['XGBoost']['model']),\n",
    "    ('knn', final_models['KNN']['model'])\n",
    "]\n",
    "\n",
    "# Create stacking regressor with linear regression as meta-learner\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Stacking Regressor...\")\n",
    "stacking_regressor.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Make predictions on validation set\n",
    "\n",
    "stacking_val_predictions = stacking_regressor.predict(X_val_final)\n",
    "\n",
    "# Calculate stacking metrics\n",
    "stacking_mse = mean_squared_error(y_val_final, stacking_val_predictions)\n",
    "stacking_mae = mean_absolute_error(y_val_final, stacking_val_predictions)\n",
    "stacking_r2 = r2_score(y_val_final, stacking_val_predictions)\n",
    "\n",
    "print(f\"\\nStacking Regressor Validation Performance:\")\n",
    "print(f\"MSE: {stacking_mse:.4f}\")\n",
    "print(f\"MAE: {stacking_mae:.4f}\")\n",
    "print(f\"R²: {stacking_r2:.4f}\")\n",
    "\n",
    "# Store ensemble results\n",
    "ensemble_results = {\n",
    "    'weighted_average': {\n",
    "        'model': None,  # No actual model, just weights\n",
    "        'weights': {'xgb': xgb_weight, 'knn': knn_weight},\n",
    "        'mse': ensemble_mse,\n",
    "        'mae': ensemble_mae,\n",
    "        'r2': ensemble_r2,\n",
    "        'predictions': weighted_val_predictions\n",
    "    },\n",
    "    'stacking': {\n",
    "        'model': stacking_regressor,\n",
    "        'mse': stacking_mse,\n",
    "        'mae': stacking_mae,\n",
    "        'r2': stacking_r2,\n",
    "        'predictions': stacking_val_predictions\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nEnsemble implementation completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092feccd",
   "metadata": {},
   "source": [
    "## Predict with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e493b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 0.0562 - Weight: 0.5335\n",
      "KNN RMSE: 0.0643 - Weight: 0.4665\n",
      "Total weight: 1.0000\n",
      "\n",
      "2. Stacking Regressor Ensemble\n",
      "------------------------------\n",
      "Training Stacking Regressor...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "xgb_final = xgb_final.fit(X_encoded, y)\n",
    "knn_pipeline_final = knn_pipeline_final.fit(X_encoded, y)\n",
    "\n",
    "xgb_val_pred = xgb_final.predict(X_test_encoded)\n",
    "knn_val_pred = knn_pipeline_final.predict(X_test_encoded)\n",
    "\n",
    "\n",
    "# Method 1: Weighted Average Ensemble\n",
    "xgb_rmse = final_models['XGBoost']['rmse']\n",
    "knn_rmse = final_models['KNN']['rmse']\n",
    "\n",
    "# Calculate weights based on RMSE (inverse relationship)\n",
    "# Lower RMSE should get higher weight\n",
    "xgb_weight = 1 / xgb_rmse\n",
    "knn_weight = 1 / knn_rmse\n",
    "\n",
    "# Normalize weights\n",
    "total_weight = xgb_weight + knn_weight\n",
    "xgb_weight = xgb_weight / total_weight\n",
    "knn_weight = knn_weight / total_weight\n",
    "\n",
    "print(f\"XGBoost RMSE: {xgb_rmse:.4f} - Weight: {xgb_weight:.4f}\")\n",
    "print(f\"KNN RMSE: {knn_rmse:.4f} - Weight: {knn_weight:.4f}\")\n",
    "print(f\"Total weight: {xgb_weight + knn_weight:.4f}\")\n",
    "\n",
    "# Create weighted ensemble predictions on validation set\n",
    "weighted_val_predictions = (xgb_weight * xgb_val_pred + \n",
    "                           knn_weight * knn_val_pred)\n",
    "\n",
    "\n",
    "# Method 2: Stacking Regressor\n",
    "print(\"\\n2. Stacking Regressor Ensemble\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create base models\n",
    "base_models = [\n",
    "    ('xgb', xgb_final),\n",
    "    ('knn', knn_pipeline_final)\n",
    "]\n",
    "\n",
    "# Create stacking regressor with linear regression as meta-learner\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Stacking Regressor...\")\n",
    "stacking_regressor.fit(X_encoded, y)\n",
    "\n",
    "# Make predictions on validation set\n",
    "\n",
    "stacking_val_predictions = stacking_regressor.predict(X_test_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8872633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Final Predictions and Submissions\n",
      "==================================================\n",
      "Stacking submission saved as 'stacking_submission_df.csv'\n",
      "Weighted submission saved as 'weighted_submission_df.csv'\n",
      "\n",
      "Test Predictions Statistics:\n",
      "========================================\n",
      "Model      Mean         Std          Min          Max         \n",
      "------------------------------------------------------------\n",
      "Stacking   0.351666     0.156860     -0.000155    0.880733    \n",
      "Weighted   0.350851     0.149560     0.032875     0.854899    \n",
      "\n",
      "Correlation between Stacking and Weighted test predictions: 0.996827\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCorrelation between Stacking and Weighted test predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_correlation\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Visualize prediction distributions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Stacking predictions distribution\u001b[39;00m\n\u001b[32m     37\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate Final Predictions and Create Submission Files\n",
    "print(\"Generating Final Predictions and Submissions\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "stacking_submission_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'accident_risk': stacking_val_predictions\n",
    "})\n",
    "\n",
    "weighted_submission_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'accident_risk': weighted_val_predictions\n",
    "})\n",
    "\n",
    "stacking_submission_df.to_csv('stacking_submission_df.csv', index=False)\n",
    "print(\"Stacking submission saved as 'stacking_submission_df.csv'\")\n",
    "\n",
    "weighted_submission_df.to_csv('weighted_submission_df.csv', index=False)\n",
    "print(\"Weighted submission saved as 'weighted_submission_df.csv'\")\n",
    "\n",
    "# Statistical comparison of test predictions\n",
    "print(f\"\\nTest Predictions Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Model':<10} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Stacking':<10} {stacking_val_predictions.mean():<12.6f} {stacking_val_predictions.std():<12.6f} {stacking_val_predictions.min():<12.6f} {stacking_val_predictions.max():<12.6f}\")\n",
    "print(f\"{'Weighted':<10} {weighted_val_predictions.mean():<12.6f} {weighted_val_predictions.std():<12.6f} {weighted_val_predictions.min():<12.6f} {weighted_val_predictions.max():<12.6f}\")\n",
    "\n",
    "# Correlation between predictions\n",
    "prediction_correlation = np.corrcoef(stacking_val_predictions, weighted_val_predictions)[0, 1]\n",
    "print(f\"\\nCorrelation between Stacking and Weighted test predictions: {prediction_correlation:.6f}\")\n",
    "\n",
    "# Visualize prediction distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Stacking predictions distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(stacking_val_predictions, bins=50, alpha=0.7, edgecolor='black', color='blue')\n",
    "plt.title('Stacking Test Predictions')\n",
    "plt.xlabel('Predicted Accident Risk')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Weighted predictions distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(weighted_val_predictions, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "plt.title('Weighted Test Predictions')\n",
    "plt.xlabel('Predicted Accident Risk')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Overlay comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(stacking_val_predictions, bins=50, alpha=0.5, edgecolor='black', color='blue', label='Stacking')\n",
    "plt.hist(weighted_val_predictions, bins=50, alpha=0.5, edgecolor='black', color='orange', label='Weighted')\n",
    "plt.title('Prediction Distributions Comparison')\n",
    "plt.xlabel('Predicted Accident Risk')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
